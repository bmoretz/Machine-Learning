---
title: ''
mainfont: Arial
fontsize: 12pt
documentclass: report
header-includes:
- \PassOptionsToPackage{table}{xcolor}
- \usepackage{caption}
- \usepackage{amssymb}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage[table]{xcolor}
- \usepackage{fancyhdr}
- \usepackage{boldline}
- \usepackage{tipa}
   \definecolor{headergrey}{HTML}{545454}
   \definecolor{msdblue}{HTML}{1C93D1}
   \pagestyle{fancy}
   \setlength\headheight{30pt}
   \rhead{\color{headergrey}\today}
   \fancyhead[L]{\color{headergrey}Moretz, Brandon}
   \fancyhead[C]{\Large\bfseries\color{headergrey}Feature and Target Engineering}
   \rfoot{\color{headergrey}\thepage}
   \lfoot{\color{headergrey}Chapter 3}
   \fancyfoot[C]{\rmfamily\color{headergrey}Hands-On Machine Learning}
geometry: left = 1cm, right = 1cm, top = 2cm, bottom = 3cm
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    fig_caption: yes
    latex_engine: xelatex
editor_options: 
  chunk_output_type: console
---


```{r knitr_setup, include = FALSE}
knitr::opts_chunk$set(
   echo = T, 
   eval = TRUE, 
   dev = 'png', 
   fig.width = 9, 
   fig.height = 3.5)

options(knitr.table.format = "latex")
```

```{r report_setup, message = FALSE, warning = FALSE, include = FALSE}
# Data Wrangling

library(data.table, quietly = TRUE, warn.conflicts = FALSE)
library(dplyr, quietly = TRUE, warn.conflicts = FALSE)
library(tinytex, quietly = TRUE, warn.conflicts = FALSE)
library(stringr, quietly = TRUE, warn.conflicts = FALSE)
library(lubridate, quietly = TRUE, warn.conflicts = FALSE)
library(reshape2, quietly = TRUE, warn.conflicts = FALSE)
library(ggrepel, quietly = TRUE, warn.conflicts = FALSE)

# Plotting / Graphics

library(ggplot2, quietly = TRUE, warn.conflicts = FALSE)
library(visdat, quietly = TRUE, warn.conflicts = FALSE)
library(ggrepel, quietly = TRUE, warn.conflicts = FALSE)
library(ggthemes, quietly = TRUE, warn.conflicts = FALSE)
library(grid, quietly = TRUE, warn.conflicts = FALSE)
library(gridExtra, quietly = TRUE, warn.conflicts = FALSE)
library(png, quietly = TRUE, warn.conflicts = FALSE)
library(extrafont, quietly = TRUE, warn.conflicts = FALSE)

# Formatting / Markdown

library(knitr, quietly = TRUE, warn.conflicts = FALSE)
library(kableExtra, quietly = TRUE, warn.conflicts = FALSE)
library(scales, quietly = TRUE, warn.conflicts = FALSE)
library(pander, quietly = TRUE, warn.conflicts = FALSE)
library(formattable, quietly = TRUE, warn.conflicts = FALSE)

# Feature Engineering
library(recipes, quietly = TRUE, warn.conflicts = FALSE)

# Utility
library(here, quietly = TRUE, warn.conflicts = FALSE)

# Resampling & Modeling
library(MASS, quietly = TRUE, warn.conflicts = FALSE)
library(rsample, quietly = TRUE, warn.conflicts = FALSE)
library(caret, quietly = TRUE, warn.conflicts = FALSE)
library(h2o, quietly = TRUE, warn.conflicts = FALSE)
library(forecast, quietly = TRUE, warn.conflicts = FALSE)

# h2o Setup

h2o.no_progress()
h2o.init()

options(tinytex.verbose = TRUE)
suppressMessages(library("tidyverse"))

pretty_kable <- function(data, title, dig = 2) {
  kable(data, caption = title, digits = dig) %>%
    kable_styling(bootstrap_options = c("striped", "hover")) %>%
      kableExtra::kable_styling(latex_options = "hold_position")
}

theme_set(theme_light())

# Theme Overrides
theme_update(axis.text.x = element_text(size = 10),
             axis.text.y = element_text(size = 10),
             plot.title = element_text(hjust = 0.5, size = 16, face = "bold", color = "darkgreen"),
             axis.title = element_text(face = "bold", size = 12, colour = "steelblue4"),
             plot.subtitle = element_text(face = "bold", size = 8, colour = "darkred"),
             legend.title = element_text(size = 12, color = "darkred", face = "bold"),
             legend.position = "right", legend.title.align=0.5,
             panel.border = element_rect(linetype = "solid", 
                                         colour = "lightgray"), 
             plot.margin = unit(c( 0.1, 0.1, 0.1, 0.1), "inches"))

data.dir <- paste0(here::here(), "/Hands-On/data/")

select <- dplyr::select # fix clash with MASS

```

```{r pander_setup, include = FALSE}

knitr::opts_chunk$set(comment = NA)

panderOptions('table.alignment.default', function(df)
    ifelse(sapply(df, is.numeric), 'right', 'left'))
panderOptions('table.split.table', Inf)
panderOptions('big.mark', ",")
panderOptions('keep.trailing.zeros', TRUE)

```

## Feature and Target Engineering

### Data Set

h2o

```{r, echo = T}
ames <- AmesHousing::make_ames()
ames.h2o <- as.h2o(ames)
```

stratified (_Sale_Price_) training sample

```{r, echo = T}
set.seed(123)

split <- initial_split(ames, prop = 0.7,
                       strata = "Sale_Price")

ames_train <- training(split)
ames_test <- testing(split)
```

log transformation (Sale_Price)

```{r, echo = T}
ames_recipe <- recipe(Sale_Price ~ ., data = ames_train) %>%
   step_log(all_outcomes())

ames_recipe
```

Box-Cox transformation (example)

```{r, echo = T}
lambda <- 3

y <- forecast::BoxCox(10, lambda)

inv_box_cox <- function(x, lambda) {
   # for Box-Cox, lambda = 0 -> log transform
   if(lambda == 0) exp(x) else (lambda*x + 1)^(1/lambda)
}

inv_box_cox(y, lambda)
```

```{r engineering-distribution-comparison, echo=TRUE, message=FALSE, warning=FALSE, fig.cap="Response variable transformations.", fig.height=3, fig.width=9}
# Log transformation
train_log_y <- log(ames_train$Sale_Price)
test_log_y  <- log(ames_train$Sale_Price)

# Box Cox transformation
lambda  <- forecast::BoxCox.lambda(ames_train$Sale_Price)
train_bc_y <- forecast::BoxCox(ames_train$Sale_Price, lambda)
test_bc_y  <- forecast::BoxCox(ames_test$Sale_Price, lambda)

# Plot differences
levs <- c("Normal", "Log_Transform", "BoxCox_Transform")
data.frame(
  Normal = ames_train$Sale_Price,
  Log_Transform = train_log_y,
  BoxCox_Transform = train_bc_y
) %>%
  gather(Transform, Value) %>%
  mutate(Transform = factor(Transform, levels = levs)) %>% 
  ggplot(aes(Value, fill = Transform)) +
    geom_histogram(show.legend = FALSE, bins = 40) +
    facet_wrap(~ Transform, scales = "free_x")
```

## Missing Values

```{r, echo = T}
sum(is.na(AmesHousing::ames_raw))
```

```{r, echo = T, fig.width=8, fig.height=3.5}
AmesHousing::ames_raw %>%
   is.na() %>%
   reshape2::melt() %>%
   ggplot(aes(Var2, Var1, fill = value)) +
      geom_raster() +
      coord_flip() +
      scale_y_continuous(NULL, expand = c(0,0)) +
      scale_fill_grey(name = "",
                      labels = c("Present",
                                 "Missing")) +
   xlab("Observations") +
   theme(axis.text.y = element_text(size = 4))
```

Missing Garage?

```{r, echo = T}
AmesHousing::ames_raw %>%
   filter(is.na(`Garage Type`)) %>%
   select(starts_with("Garage"))

```

Missing values w/cluster (_visdat_)

```{r, echo = T, fig.width=8, fig.height=4}
vis_miss(AmesHousing::ames_raw, cluster = T)
```

### Missing Value Imputation

basic descriptive statistic

```{r, echo = T}
ames_recipe %>%
   step_medianimpute(Gr_Liv_Area)
```

KNN approach (typical k = 5-10)

```{r, echo = T}
ames_recipe %>%
   step_knnimpute(all_predictors(), neighbors = 6)
```

```{r engineering-imputation-examples, echo=TRUE, fig.cap="Comparison of three different imputation methods. The red points represent actual values which were removed and made missing and the blue points represent the imputed values. Estimated statistic imputation methods (i.e. mean, median) merely predict the same value for each observation and can reduce the signal between a feature and the response; whereas KNN and tree-based procedures tend to maintain the feature distribution and relationship."}
impute_ames <- ames_train
set.seed(123)
index <- sample(seq_along(impute_ames$Gr_Liv_Area), 50)
actuals <- ames_train[index, ]
impute_ames$Gr_Liv_Area[index] <- NA

p1 <- ggplot() +
  geom_point(data = impute_ames, aes(Gr_Liv_Area, Sale_Price), alpha = .2) +
  geom_point(data = actuals, aes(Gr_Liv_Area, Sale_Price), color = "red") +
  scale_x_log10(limits = c(300, 5000)) +
  scale_y_log10(limits = c(10000, 500000)) +
  ggtitle("Actual values")

# Mean imputation
mean_juiced <- recipe(Sale_Price ~ ., data = impute_ames) %>%
  step_meanimpute(Gr_Liv_Area) %>%
  prep(training = impute_ames, retain = TRUE) %>%
  juice()
mean_impute <- mean_juiced[index, ]
  
p2 <- ggplot() +
  geom_point(data = actuals, aes(Gr_Liv_Area, Sale_Price), color = "red") +
  geom_point(data = mean_impute, aes(Gr_Liv_Area, Sale_Price), color = "blue") +
  scale_x_log10(limits = c(300, 5000)) +
  scale_y_log10(limits = c(10000, 500000)) +
  ggtitle("Mean Imputation")

# KNN imputation
knn_juiced <- recipe(Sale_Price ~ ., data = impute_ames) %>%
  step_knnimpute(Gr_Liv_Area) %>%
  prep(training = impute_ames, retain = TRUE) %>%
  juice()
knn_impute <- knn_juiced[index, ]
  
p3 <- ggplot() +
  geom_point(data = actuals, aes(Gr_Liv_Area, Sale_Price), color = "red") +
  geom_point(data = knn_impute, aes(Gr_Liv_Area, Sale_Price), color = "blue") +
  scale_x_log10(limits = c(300, 5000)) +
  scale_y_log10(limits = c(10000, 500000)) +
  ggtitle("KNN Imputation")

# Bagged imputation
bagged_juiced <- recipe(Sale_Price ~ ., data = impute_ames) %>%
  step_bagimpute(Gr_Liv_Area) %>%
  prep(training = impute_ames, retain = TRUE) %>%
  juice()
bagged_impute <- bagged_juiced[index, ]
  
p4 <- ggplot() +
  geom_point(data = actuals, aes(Gr_Liv_Area, Sale_Price), color = "red") +
  geom_point(data = bagged_impute, aes(Gr_Liv_Area, Sale_Price), color = "blue") +
  scale_x_log10(limits = c(300, 5000)) +
  scale_y_log10(limits = c(10000, 500000)) +
  ggtitle("Bagged Trees Imputation")

gridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2)
```

Increase in training time by model type:

```{r, echo = T, fig.width=9, fig.height=4}
model_results <- read_csv(paste0(data.dir, "feature-selection-impacts-results.csv")) %>%
   mutate(type = case_when(
      model %in% c("lm", "pcr", "pls", "glmnet", "lasso") ~ "Linear Models",
      model %in% c("earth", "svmLinear", "nn") ~ "Non-linear Models",
      TRUE ~ "Tree-based Models"
   )) %>%
   mutate(model = case_when(
      model == "lm" ~ "Linear regression",
      model == "earth" ~ "Multivariate adaptive regression splines",
      model == "gbm" ~ "Gradient boosting machines",
      model == "glmnet" ~ "Elastic net",
      model == "lasso" ~ "Lasso",
      model == "nn" ~ "Neural net",
      model == "pcr" ~ "Principal component regression",
      model == "pls" ~ "Partial least squares",
      model == "ranger" ~ "Random forest",
      TRUE ~ "Support vector machine"
  ))

ggplot(model_results, aes(NIP, RMSE, color = model, lty = model)) +
  geom_line() +
  geom_point() +
  facet_wrap(~ type, nrow = 1) +
  xlab("Number of additional non-informative predictors")
```

```{r engineering-impact-on-time, echo=TRUE, fig.width=10, fig.height=3.5, fig.cap="Impact in model training time as non-informative predictors are added."}
model_results %>%
  group_by(model) %>%
  mutate(
    time_impact = time / first(time),
    time_impact = time_impact - 1
  ) %>%
  ggplot(aes(NIP, time_impact, color = model, lty = model)) +
    geom_line() +
    geom_point() +
    facet_wrap(~ type, nrow = 1) +
    scale_y_continuous("Percent increase in training time", 
                       labels = scales::percent) +
    xlab("Number of additional non-informative predictors")
```

Rules of thumb for zero variance features:

+ The fraction of unique values over the sample size is low (say < 10%)
+ The ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value is large (say > 20%)

If both of these criteria are met, then it is often advantageous to remove them from the model.

```{r, echo = T}
caret::nearZeroVar(ames_train, saveMetrics = T) %>%
   rownames_to_column() %>%
   filter(nzv)
```

## Numeric Feature Engineering

Skewness can have a drastic impact on the performance of GLMs & regularized models.

Non-parametric models are rarely affected by skewed features; however, normalizing features will not have a negative effect on these models' performance. For example, normalizing features will only shift the optimal split points in tree-based algoirthms. Consequently, when in doubt, normalize.

### Skewness

```{r engineering-normalizing}
# Normalize all numeric columns
recipe(Sale_Price ~ ., data = ames_train) %>%
  step_YeoJohnson(all_numeric())                 
```

### Standardization

```{r, echo = T}
ames_recipe %>%
   step_center(all_numeric(), -all_outcomes()) %>%
   step_scale(all_numeric(), -all_outcomes())
```

```{r engineering-standardizing, echo=TRUE, fig.height=3, fig.cap="Standardizing features allows all features to be compared on a common value scale regardless of their real value differences."}
set.seed(123)
x1 <- tibble(
   variable = "x1",
   `Real Value` = runif(25, min = -30, max = 5),
   `Standardized Value` = scale(`Real Value`) %>% as.numeric()
   
)

set.seed(456)
x2 <- tibble(
  variable = "x2",
  `Real value` = rlnorm(25, log(25)),
  `Standardized value` = scale(`Real value`) %>% as.numeric()
)

set.seed(789)
x3 <- tibble(
  variable = "x3",
  `Real value` = rnorm(25, 150, 15),
  `Standardized value` = scale(`Real value`) %>% as.numeric()
)

x1 %>%
  bind_rows(x2) %>%
  bind_rows(x3) %>%
  gather(key, value, -variable) %>%
  mutate(variable = factor(variable, levels = c("x3", "x2", "x1"))) %>%
  ggplot(aes(value, variable)) +
    geom_point(alpha = .6) +
    facet_wrap(~ key, scales = "free_x") +
    ylab("Feature") +
    xlab("Value")
```

## Categorical Feature Engineering

### Lumping

When a feature contains levels that have few observations.

For example:

```{r, echo = T}
count(ames_train, Neighborhood) %>% arrange(n)
```

```{r, echo = T}
count(ames_train, Screen_Porch) %>% arrange(n)
```

We can benefit from lumping these together into an "other" category when they contain less than 10% of the training sample.

__Note: This can have an adverse effect on performance__

```{r, echo = T}
lumping <- recipe(Sale_Price ~., data = ames_train) %>%
   step_other(Neighborhood, threshold = 0.01,
              other = "other") %>%
   step_other(Screen_Porch, threshold = 0.1,
              other = ">0")

apply_2_training <- prep(lumping, training = ames_train) %>%
  bake(ames_train)

# New distribution of Neighborhood
count(apply_2_training, Neighborhood) %>% arrange(n)

# New distribution of Screen_Porch
count(apply_2_training, Screen_Porch) %>% arrange(n)
```

```{r, echo = T}
dat <- data.table(id = 1:9, x = rep(c("a", "b" , "c"), 3))
dat

# full-rank
dat[, .(id, 
        `X = a` = as.numeric(x == "a"), 
        `X = b` = as.numeric(x == "b"), 
        `X = c` = as.numeric(x == "c")) ]

# one-hot (leave one out)
dat[, .(id, 
        `X = a` = as.numeric(x == "a"), 
        `X = b` = as.numeric(x == "b")) ]

```

```{r, echo = T}
# Lump levels for two features
recipe(Sale_Price ~., data = ames_train) %>%
   step_dummy(all_nominal(), one_hot = T)
```

### Label Encoding

_Label encoding_ is a pure numeric conversion of the levels of a categorical variable.

For example, the _MS_SubClass_ variable has 16 levels, which we can reencode numerically.

__Important:__ The features will be treated as ordered (ordnal encoding), so if the feature is not natually ordered, this will have a poor impact on the model.

```{r, echo = T}
count(ames_train, MS_SubClass)
```

```{r, echo = T}
# Label encoded
recipe(Sale_Price ~ ., data = ames_train) %>%
   step_integer(MS_SubClass) %>%
   prep(ames_train) %>%
   bake(ames_train) %>%
   count(MS_SubClass)
```

Examples of ordnal features:

```{r, echo = T}
ames_train %>% select(contains("Qual"))
```

```{r, echo = T}
count(ames_train, Overall_Qual)
```

```{r, echo = T}
# Label encoded
recipe(Sale_Price ~., data = ames_train) %>%
   step_integer(Overall_Qual) %>%
   prep(ames_train) %>%
   bake(ames_train) %>%
   count(Overall_Qual)
```

### Alternatives

Target encoding:

```{r engineering-target-encoding, echo=TRUE, fig.cap='Example of target encoding the Neighborhood feature of the Ames housing data set.'}
ames_train %>%
  group_by(Neighborhood) %>%
  summarize(`Avg Sale_Price` = mean(Sale_Price, na.rm = TRUE)) %>%
  head(10) %>%
  kable(caption = "Example of target encoding the Neighborhood feature of the Ames housing data set.") %>%
  kable_styling(bootstrap_options = "striped", full_width = TRUE)
```

```{r engineering-proportion-encoding, echo=TRUE, fig.cap='Example of categorical proportion encoding the Neighborhood feature of the Ames housing data set.'}
ames_train %>%
  count(Neighborhood) %>%
  mutate(Proportion = n / sum(n)) %>%
  select(-n) %>%
  head(10) %>%
  kable(caption = 'Example of categorical proportion encoding the Neighborhood feature of the Ames housing data set.') %>%
  kable_styling(bootstrap_options = "striped", full_width = TRUE)
```

## Dimension Reduction

Example PCA using _resample_ package.

```{r engineering-pca}
recipe(Sale_Price ~ ., data = ames_train) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  step_pca(all_numeric(), threshold = .95)
```

## Full Recipe

Full blueprint recipe applied to training and test data.

```{r, echo = T}
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
   step_nzv(all_nominal()) %>%
   step_integer(matches("Qual|Cond|QC|Qu")) %>%
   step_center(all_numeric(), -all_outcomes()) %>%
   step_scale(all_numeric(), -all_outcomes()) %>%
   step_pca(all_numeric(), -all_outcomes())

blueprint

prepare <- prep(blueprint, training = ames_train)
prepare

baked_train <- bake(prepare, new_data = ames_train)
baked_test <- bake(prepare, new_data = ames_test)

baked_train
```

Full recipe with cross-validation & grid search using carat.

```{r engineering-knn-blueprint}
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_nzv(all_nominal()) %>%
  step_integer(matches("Qual|Cond|QC|Qu")) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE)
```

```{r engineering-knn-with-blueprint}
# Specify resampling plan
cv <- trainControl(
  method = "repeatedcv", 
  number = 10, 
  repeats = 5
)

# Construct grid of hyperparameter values
hyper_grid <- expand.grid(k = seq(2, 25, by = 1))

# Tune a knn model using grid search
knn_fit2 <- train(
  blueprint, 
  data = ames_train, 
  method = "knn", 
  trControl = cv, 
  tuneGrid = hyper_grid,
  metric = "RMSE"
)
```

```{r engineering-knn-with-blueprint-assess, fig.height=3, fig.cap="Results from the same grid search performed in Section 2.7 but with feature engineering performed within each resample."}
# print model results
knn_fit2
# plot cross validation results
ggplot(knn_fit2)
```