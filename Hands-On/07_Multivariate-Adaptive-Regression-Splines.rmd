---
title: ''
mainfont: Arial
fontsize: 12pt
documentclass: report
header-includes:
- \PassOptionsToPackage{table}{xcolor}
- \usepackage{caption}
- \usepackage{amssymb}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage[table]{xcolor}
- \usepackage{fancyhdr}
- \usepackage{boldline}
- \usepackage{tipa}
   \definecolor{headergrey}{HTML}{545454}
   \definecolor{msdblue}{HTML}{1C93D1}
   \pagestyle{fancy}
   \setlength\headheight{30pt}
   \rhead{\color{headergrey}\today}
   \fancyhead[L]{\color{headergrey}Moretz, Brandon}
   \fancyhead[C]{\Large\bfseries\color{headergrey}Multivariate Adaptive Regression Splines}
   \rfoot{\color{headergrey}\thepage}
   \lfoot{\color{headergrey}Chapter 7}
   \fancyfoot[C]{\rmfamily\color{headergrey}Hands-On Machine Learning}
geometry: left = 1cm, right = 1cm, top = 2cm, bottom = 3cm
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    fig_caption: yes
    latex_engine: xelatex
editor_options: 
  chunk_output_type: console
fig_width: 9
fig_height: 3.5
---


```{r knitr_setup, include = FALSE}
knitr::opts_chunk$set(
   echo = T, 
   eval = TRUE, 
   dev = 'png', 
   fig.width = 9, 
   fig.height = 3.5)

options(knitr.table.format = "latex")
```

```{r report_setup, message = FALSE, warning = FALSE, include = FALSE}
# Data Wrangling

library(data.table, quietly = TRUE, warn.conflicts = FALSE)
library(dplyr, quietly = TRUE, warn.conflicts = FALSE)
library(tinytex, quietly = TRUE, warn.conflicts = FALSE)
library(stringr, quietly = TRUE, warn.conflicts = FALSE)
library(lubridate, quietly = TRUE, warn.conflicts = FALSE)
library(reshape2, quietly = TRUE, warn.conflicts = FALSE)
library(ggrepel, quietly = TRUE, warn.conflicts = FALSE)
library(tidyr, quietly = TRUE, warn.conflicts = FALSE)

# Plotting / Graphics

library(ggplot2, quietly = TRUE, warn.conflicts = FALSE)
library(visdat, quietly = TRUE, warn.conflicts = FALSE)
library(ggrepel, quietly = TRUE, warn.conflicts = FALSE)
library(ggthemes, quietly = TRUE, warn.conflicts = FALSE)
library(grid, quietly = TRUE, warn.conflicts = FALSE)
library(gridExtra, quietly = TRUE, warn.conflicts = FALSE)
library(png, quietly = TRUE, warn.conflicts = FALSE)
library(extrafont, quietly = TRUE, warn.conflicts = FALSE)
library(pdp, quietly = TRUE, warn.conflicts = FALSE)
library(ROCR, quietly = TRUE, warn.conflicts = FALSE)

# Formatting / Markdown

library(knitr, quietly = TRUE, warn.conflicts = FALSE)
library(kableExtra, quietly = TRUE, warn.conflicts = FALSE)
library(scales, quietly = TRUE, warn.conflicts = FALSE)
library(pander, quietly = TRUE, warn.conflicts = FALSE)
library(formattable, quietly = TRUE, warn.conflicts = FALSE)

# Feature Engineering
library(recipes, quietly = TRUE, warn.conflicts = FALSE)

# Utility
library(here, quietly = TRUE, warn.conflicts = FALSE)

# Resampling & Modeling
library(MASS, quietly = TRUE, warn.conflicts = FALSE)
library(rsample, quietly = TRUE, warn.conflicts = FALSE)
library(caret, quietly = TRUE, warn.conflicts = FALSE)
library(h2o, quietly = TRUE, warn.conflicts = FALSE)
library(forecast, quietly = TRUE, warn.conflicts = FALSE)
library(vip, quietly = TRUE, warn.conflicts = FALSE)
library(glmnet, quietly = TRUE, warn.conflicts = FALSE)
library(earth, quietly = TRUE, warn.conflicts = FALSE)
library(mda, quietly = TRUE, warn.conflicts = FALSE)

# h2o Setup

h2o.no_progress()
h2o.init(strict_version_check = F)

options(tinytex.verbose = TRUE)
suppressMessages(library("tidyverse"))

pretty_kable <- function(data, title, dig = 2) {
  kable(data, caption = title, digits = dig) %>%
    kable_styling(bootstrap_options = c("striped", "hover")) %>%
      kableExtra::kable_styling(latex_options = "hold_position")
}

theme_set(theme_light())

# Theme Overrides
theme_update(axis.text.x = element_text(size = 10),
             axis.text.y = element_text(size = 10),
             plot.title = element_text(hjust = 0.5, size = 16, face = "bold", color = "darkgreen"),
             axis.title = element_text(face = "bold", size = 12, colour = "steelblue4"),
             plot.subtitle = element_text(face = "bold", size = 8, colour = "darkred"),
             legend.title = element_text(size = 12, color = "darkred", face = "bold"),
             legend.position = "right", legend.title.align=0.5,
             panel.border = element_rect(linetype = "solid", 
                                         colour = "lightgray"), 
             plot.margin = unit(c( 0.1, 0.1, 0.1, 0.1), "inches"))

data.dir <- paste0(here::here(), "/Hands-On/data/")

select <- dplyr::select # fix clash with MASS

# Set global R options
options(scipen = 999)
```

```{r pander_setup, include = FALSE}

knitr::opts_chunk$set(comment = NA)

panderOptions('table.alignment.default', function(df)
    ifelse(sapply(df, is.numeric), 'right', 'left'))
panderOptions('table.split.table', Inf)
panderOptions('big.mark', ",")
panderOptions('keep.trailing.zeros', TRUE)

```

## Multivariate Adaptive Regression Splines

### Data Sets

```{r, echo = T}
attrition <- attrition %>% mutate_if(is.ordered, factor, order = F)
attrition.h2o <- as.h2o(attrition)
```

```{r, echo = T}
set.seed(123)

ames <- AmesHousing::make_ames()
ames.h2o <- as.h2o(ames)

ames.split <- initial_split(ames, prop =.7, strata = "Sale_Price")

ames.train <- training(ames.split)
ames.test <- testing(ames.split)
```

### Overview

Linear models assume the underlying phenomena we are modeling is intrinsically linear which is not usually true. Multivariate adaptive regression splines (MARS) allow us to model non-linear relationships.

Basic strategies for modeling non-linear fits include polynomial regression and step-wise models.

Visually:

```{r}
set.seed(123)

x <- seq(from = 0, to = 2 * pi, length = 500)
y <- sin(x) + rnorm(length(x), sd = .3)

df <- data.table(x, y) %>%
   filter(x < 6)

p1 <- ggplot(df, aes(x, y)) +
   geom_point(alpha = .25) +
   geom_smooth(method = "lm", se = F) +
   ggtitle("(A) Assumed Linear Relationship")

p2 <- ggplot(df, aes(x, y)) +
   geom_point(alpha = .25) +
   geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 2, raw = T)) +
   ggtitle("(B) Degree-2 Polynomial Regression")

p3 <- ggplot(df, aes(x, y)) +
   geom_point(alpha = .25) +
   geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 3, raw = T)) +
   ggtitle("(C) Degree-3 Polynomial Regression")

# fit step function model (6 steps)

step_fit <- lm(y ~ cut(x, 5), data = df)
step_pred <- predict(step_fit, df)

p4 <- ggplot(df, aes(x, y)) +
   geom_point(alpha = .25) +
   geom_line(aes(y = step_pred), size = 1, color = "blue") +
   ggtitle("(D) Step Function Regression")

gridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2)
```

### Multivariate Adaptive Regression Splines (MARS)

Similar to the step-wise approach, we can access "knots" in the data to model this behavior.

MARS models with _mda_ package.


```{r}
mars1 <- mda::mars(df$x, df$y, nk = 3, prune = F)

p1 <- df %>%
   mutate(predicted = as.vector(mars1$fitted.values)) %>%
   ggplot(aes(x, y)) +
   geom_point(size = 1, alpha = .2) +
   geom_line(aes(y = predicted), size = 1, color = "blue") +
   ggtitle("(A) One Knot")

mars2 <- mda::mars(df$x, df$y, nk = 5, prune = F)

p2 <- df %>%
   mutate(predicted = as.vector(mars2$fitted.values)) %>%
   ggplot(aes(x, y)) +
   geom_point(size = 1, alpha = .2) +
   geom_line(aes(y = predicted), size = 1, color = "blue") +
   ggtitle("(B) Two Knots")

mars3 <- mda::mars(df$x, df$y, nk = 7, prune = F)

p3 <- df %>%
   mutate(predicted = as.vector(mars3$fitted.values)) %>%
   ggplot(aes(x, y)) +
   geom_point(size = 1, alpha = .2) +
   geom_line(aes(y = predicted), size = 1, color = "blue") +
   ggtitle("(C) Three Knots")

mars4 <- mda::mars(df$x, df$y, nk = 9, prune = F)

p4 <- df %>%
   mutate(predicted = as.vector(mars4$fitted.values)) %>%
      ggplot(aes(x, y)) +
   geom_point(size = 1, alpha = .2) +
   geom_line(aes(y = predicted), size = 1, color = "blue") +
   ggtitle("(D) Four Knots")

gridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2)
```

### Fitting a basic MARS model

```{r}
mars1 <- earth(
   Sale_Price ~ .,
   data = ames.train
)

print(mars1)
```

```{r}
summary(mars1) %>% .$coefficients %>% head(10)
```

```{r}
plot(mars1, which = 1)
plot(mars1, which = 2)
```

```{r}
mars2 <- earth(
   Sale_Price ~.,
   data = ames.train,
   degree = 2
)

summary(mars2) %>% .$coefficients %>% head(10)
```

```{r}
plot(mars2, which = 1)
```

### Tuning

As always, we will use a cross-validated grid search procedure to tune the hyperparameters.

First pass:

```{r}
hyper.grid <- expand.grid(
   degree = 1:3,
   nprune = seq(2, 100, length.out = 10) %>% floor()
)

head(hyper.grid)
```

```{r}
set.seed(123)

suppressWarnings(print({
cv.mars <- train(
   x = ames.train %>% select(-Sale_Price),
   y = ames.train$Sale_Price,
   method = "earth",
   metric = "RMSE",
   trControl = trainControl(method = "cv", number = 10),
   tuneGrid = hyper.grid
)}))

cv.mars$bestTune

cv.mars$results %>% 
   as_tibble() %>% 
   arrange(RMSE)

```

```{r}
ggplot(cv.mars) +
   scale_y_continuous(labels = scales::comma)
```

Refinement:

```{r}
refine.grid <- expand.grid(
   degree = 1,
   nprune = seq(from = 30, to = 45)
)

suppressWarnings(print({
   cv.mars2 <- train(
      x = ames.train %>% select(-Sale_Price),
      y = ames.train$Sale_Price,
      method = "earth",
      metric = "RMSE",
      trControl = trainControl(method = "cv", number = 10),
      tuneGrid = refine.grid
   )
}))

cv.mars2$bestTune

ggplot(cv.mars2) +
   scale_y_continuous(labels = scales::comma)

```

### Feature Interpretation

Variable Importance Plots

```{r, fig.height=6}
p1 <- vip::vip(cv.mars, num_features = 40, bar = F, value = "gcv") +
   ggtitle("GCV")

p2 <- vip::vip(cv.mars2, num_features = 40, bar = F, value = "rss") +
   ggtitle("RSS")

gridExtra::grid.arrange(p1, p2, nrow = 2)
```

```{r, fig.height=6}
p1 <- vip::vip(cv.mars2, num_features = 40, bar = F, value = "gcv") +
   ggtitle("GCV")

p2 <- vip::vip(cv.mars2, num_features = 40, bar = F, value = "rss") +
   ggtitle("RSS")

gridExtra::grid.arrange(p1, p2, nrow = 2)
```

```{r}
# extract coefficients, covert to tidy & filter for interaction

cv.mars2$finalModel %>%
   coef() %>%
   broom::tidy()
```

```{r}
# Construct partial dependence plots
p1 <- pdp::partial(cv.mars2, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% 
  autoplot() +
   scale_y_continuous(labels = scales::comma)

p2 <- pdp::partial(cv.mars2, pred.var = "Year_Built", grid.resolution = 10) %>% 
  autoplot() +
   scale_y_continuous(labels = scales::comma)

p3 <- pdp::partial(cv.mars2, pred.var = c("Gr_Liv_Area", "Year_Built"), 
              grid.resolution = 10) %>% 
  plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, colorkey = TRUE, 
              screen = list(z = -20, x = -60))

# Display plots side by side
gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
```

### Attrition data

```{r}
df <- rsample::attrition %>% mutate_if(is.ordered, factor, order = F)

# Create training (70%) and test (30%) sets for the attrition data.
set.seed(123)

churn.split <- initial_split(df, prop = .7, strata = "Attrition")
churn.train <- training(churn.split)
churn.test <- testing(churn.split)

set.seed(123)

suppressWarnings(print({
tuned.mars <- train(
   x = subset(churn.train, select = -Attrition),
   y = churn.train$Attrition,
   method = "earth",
   trControl = trainControl(method = "cv", number = 10),
   tuneGrid = hyper.grid
)}))

tuned.mars$bestTune
```

```{r}
ggplot(tuned.mars)
```

```{r}
# train logistic regression model
set.seed(123)

glm.mod <- train(
  Attrition ~ ., 
  data = churn.train, 
  method = "glm",
  family = "binomial",
  preProc = c("zv", "center", "scale"),
  trControl = trainControl(method = "cv", number = 10)
)

# train regularized logistic regression model
set.seed(123)

penalized.mod <- train(
  Attrition ~ ., 
  data = churn.train, 
  method = "glmnet",
  family = "binomial",
  preProc = c("zv", "center", "scale"),
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 10
)

# extract out of sample performance measures
summary(resamples(list(
  Logistic_model = glm.mod, 
  Elastic_net = penalized.mod,
  MARS_model = tuned.mars
  )))$statistics$Accuracy %>%
  kableExtra::kable(caption = "Cross-validated accuracy results for tuned MARS and regression models.") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))

```

```{r}
# clean up
rm(list = ls())
```

