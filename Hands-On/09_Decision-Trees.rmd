---
title: ''
mainfont: Arial
fontsize: 12pt
documentclass: report
header-includes:
- \PassOptionsToPackage{table}{xcolor}
- \usepackage{caption}
- \usepackage{amssymb}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage[table]{xcolor}
- \usepackage{fancyhdr}
- \usepackage{boldline}
- \usepackage{tipa}
   \definecolor{headergrey}{HTML}{545454}
   \definecolor{msdblue}{HTML}{1C93D1}
   \pagestyle{fancy}
   \setlength\headheight{30pt}
   \rhead{\color{headergrey}\today}
   \fancyhead[L]{\color{headergrey}Moretz, Brandon}
   \fancyhead[C]{\Large\bfseries\color{headergrey}Decision Trees}
   \rfoot{\color{headergrey}\thepage}
   \lfoot{\color{headergrey}Chapter 9}
   \fancyfoot[C]{\rmfamily\color{headergrey}Hands-On Machine Learning}
geometry: left = 1cm, right = 1cm, top = 2cm, bottom = 3cm
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    fig_caption: yes
    latex_engine: xelatex
editor_options: 
  chunk_output_type: console
fig_width: 9
fig_height: 3.5
---


```{r knitr_setup, include = FALSE}
knitr::opts_chunk$set(
   echo = T, 
   eval = TRUE, 
   dev = 'png', 
   fig.width = 9, 
   fig.height = 3.5)

options(knitr.table.format = "latex")
```

```{r report_setup, message = FALSE, warning = FALSE, include = FALSE}
# Data Wrangling

library(data.table, quietly = TRUE, warn.conflicts = FALSE)
library(dplyr, quietly = TRUE, warn.conflicts = FALSE)
library(tinytex, quietly = TRUE, warn.conflicts = FALSE)
library(stringr, quietly = TRUE, warn.conflicts = FALSE)
library(lubridate, quietly = TRUE, warn.conflicts = FALSE)
library(reshape2, quietly = TRUE, warn.conflicts = FALSE)
library(ggrepel, quietly = TRUE, warn.conflicts = FALSE)
library(tidyr, quietly = TRUE, warn.conflicts = FALSE)

# Plotting / Graphics

library(ggplot2, quietly = TRUE, warn.conflicts = FALSE)
library(visdat, quietly = TRUE, warn.conflicts = FALSE)
library(ggrepel, quietly = TRUE, warn.conflicts = FALSE)
library(ggthemes, quietly = TRUE, warn.conflicts = FALSE)
library(grid, quietly = TRUE, warn.conflicts = FALSE)
library(gridExtra, quietly = TRUE, warn.conflicts = FALSE)
library(png, quietly = TRUE, warn.conflicts = FALSE)
library(extrafont, quietly = TRUE, warn.conflicts = FALSE)
library(pdp, quietly = TRUE, warn.conflicts = FALSE)
library(ROCR, quietly = TRUE, warn.conflicts = FALSE)
library(ggmap, quietly = T, warn.conflicts = FALSE)
library(rpart.plot, quietly = T, warn.conflicts = F)

# Formatting / Markdown

library(knitr, quietly = TRUE, warn.conflicts = FALSE)
library(kableExtra, quietly = TRUE, warn.conflicts = FALSE)
library(scales, quietly = TRUE, warn.conflicts = FALSE)
library(pander, quietly = TRUE, warn.conflicts = FALSE)
library(formattable, quietly = TRUE, warn.conflicts = FALSE)

# Feature Engineering
library(recipes, quietly = TRUE, warn.conflicts = FALSE)

# Utility
library(here, quietly = TRUE, warn.conflicts = FALSE)

# Resampling & Modeling
library(MASS, quietly = TRUE, warn.conflicts = FALSE)
library(rsample, quietly = TRUE, warn.conflicts = FALSE)
library(caret, quietly = TRUE, warn.conflicts = FALSE)
library(h2o, quietly = TRUE, warn.conflicts = FALSE)
library(forecast, quietly = TRUE, warn.conflicts = FALSE)
library(vip, quietly = TRUE, warn.conflicts = FALSE)
library(glmnet, quietly = TRUE, warn.conflicts = FALSE)
library(earth, quietly = TRUE, warn.conflicts = FALSE)
library(rpart, quietly = TRUE, warn.conflicts = F)

# h2o Setup

h2o.no_progress()
h2o.init(strict_version_check = F)

options(tinytex.verbose = TRUE)
suppressMessages(library("tidyverse"))

pretty_kable <- function(data, title, dig = 2) {
  kable(data, caption = title, digits = dig) %>%
    kable_styling(bootstrap_options = c("striped", "hover")) %>%
      kableExtra::kable_styling(latex_options = "hold_position")
}

theme_set(theme_light())

# Theme Overrides
theme_update(axis.text.x = element_text(size = 10),
             axis.text.y = element_text(size = 10),
             plot.title = element_text(hjust = 0.5, size = 16, face = "bold", color = "darkgreen"),
             axis.title = element_text(face = "bold", size = 12, colour = "steelblue4"),
             plot.subtitle = element_text(face = "bold", size = 8, colour = "darkred"),
             legend.title = element_text(size = 12, color = "darkred", face = "bold"),
             legend.position = "right", legend.title.align=0.5,
             panel.border = element_rect(linetype = "solid", 
                                         colour = "lightgray"), 
             plot.margin = unit(c( 0.1, 0.1, 0.1, 0.1), "inches"))

data.dir <- paste0(here::here(), "/Hands-On/data/")

select <- dplyr::select # fix clash with MASS

# Set global R options
options(scipen = 999)
```

```{r pander_setup, include = FALSE}

knitr::opts_chunk$set(comment = NA)

panderOptions('table.alignment.default', function(df)
    ifelse(sapply(df, is.numeric), 'right', 'left'))
panderOptions('table.split.table', Inf)
panderOptions('big.mark', ",")
panderOptions('keep.trailing.zeros', TRUE)

```

##  Decision Trees

### Data Sets

Attrition

```{r, echo = T}
attrition <- attrition %>% mutate_if(is.ordered, factor, order = F)
attrition.h2o <- as.h2o(attrition)

churn <- initial_split(attrition, prop = .7, strata = "Attrition")
churn.train <- training(churn)
churn.test <- testing(churn)
```

Ames, Iowa housing data.

```{r, echo = T}
set.seed(123)

ames <- AmesHousing::make_ames()
ames.h2o <- as.h2o(ames)

ames.split <- initial_split(ames, prop =.7, strata = "Sale_Price")

ames.train <- training(ames.split)
ames.test <- testing(ames.split)
```

### Decision Trees

A decision tree is a flowchart-like structure in which each internal node represents a "test" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The most common type of decision tree is a *c*lassification *a*nd *r*egresion *t*ree (CART).

### Partitioning

CART uses _binary recursive partitioning_, where the objective at each node is to find the "best" feature ($x_i$) to partition the remaining data into one of two regions ($R_1, R_2$) such that the overall error between the actual response ($y_i$) and the predicted constant ($c_i$) is minimized.

For regression, the objective is to minimize the total SSE:

$SSE = \sum_{i \in R_1}{(y_i - c_1)^2} + \sum_{i \in R_2}{(y_i - c_2)^2}$

For classification problems, the partitioning is usually made to maximize the reduction in cross-enthropy or the Gini index.

For example, say we have data generated from a simple sin function with Gaussian noise: $Y_i \stackrel{i.i.d}{\sim} N(sin(X_i, \sigma^2)$

Data & model:

```{r}
set.seed(1112)

df <- tibble::tibble(
   x = seq(from = 0, to = 2 * pi, length = 500),
   y = sin(x) + rnorm(length(x), sd = .5),
   truth = sin(x)
)

# run decision stump model
ctrl <- list(cp = 0, minbucket = 5, maxdepth = 1)
fit <- rpart(y ~ x, data = df, control = ctrl)

# plot tree
rpart.plot(fit)
```

Decision Boundry:

```{r}
df %>%
   mutate(pred = predict(fit, df)) %>%
   ggplot(aes(x, y)) +
      geom_point(alpha = .2, size = 1) +
      geom_line(aes(x, y = truth), color = "blue", size = .75) +
      geom_line(aes(y = pred), color = "red", size = .75) +
      geom_segment(x = 3.1, xend = 3.1, y = -Inf, yend = -.95,
                   arrow = arrow(length = unit(0.25, "cm")), size = .25) +
      annotate("text", x = 3.1, y = -Inf, label = "split", hjust = 1.2, vjust = -1, size = 3) +
      geom_segment(x = 5.5, xend = 6, y = 2, yend = 2, size = .75, color = "blue") +
      geom_segment(x = 5.5, xend = 6, y = 1.7, yend = 1.7, size = .75, color = "red") +
      annotate("text", x = 5.3, y = 2, label = "truth", hjust = 1, size = 3, color = "blue") +
      annotate("text", x = 5.3, y = 1.7, label = "decision boundary", hjust = 1, size = 3, color = "red")
```

Depth 3 decision tree:

```{r}
# fit depth 3 decision tree
ctrl <- list(cp = 0, minbucket = 5, maxdepth = 3)
fit <- rpart(y ~ x, data = df, control = ctrl)
rpart.plot(fit)
```

Decision Boundry:

```{r}
# plot decision boundary
df %>%
  mutate(pred = predict(fit, df)) %>%
  ggplot(aes(x, y)) +
  geom_point(alpha = .2, size = 1) +
  geom_line(aes(x, y = truth), color = "blue", size = .75) +
  geom_line(aes(y = pred), color = "red", size = .75)
```

IRIS dataset:

```{r}
# decision tree
iris_fit <- rpart(Species ~ Sepal.Length + Sepal.Width, data = iris)
rpart.plot(iris_fit)
```

```{r}
# decision boundary
ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species, shape = Species)) +
  geom_point(show.legend = FALSE) +
  annotate("rect", xmin = -Inf, xmax = 5.44, ymin = 2.8, ymax = Inf, alpha = .75, fill = "orange") +
  annotate("text", x = 4.0, y = 4.4, label = "setosa", hjust = 0, size = 3) +
  annotate("rect", xmin = -Inf, xmax = 5.44, ymin = 2.79, ymax = -Inf, alpha = .75, fill = "grey") +
  annotate("text", x = 4.0, y = 2, label = "versicolor", hjust = 0, size = 3) +
  annotate("rect", xmin = 5.45, xmax = 6.15, ymin = 3.1, ymax = Inf, alpha = .75, fill = "orange") +
  annotate("text", x = 6, y = 4.4, label = "setosa", hjust = 1, vjust = 0, size = 3) +
  annotate("rect", xmin = 5.45, xmax = 6.15, ymin = 3.09, ymax = -Inf, alpha = .75, fill = "grey") +
  annotate("text", x = 6.15, y = 2, label = "versicolor", hjust = 1, vjust = 0, fill = "grey", size = 3) +
  annotate("rect", xmin = 6.16, xmax = Inf, ymin = -Inf, ymax = Inf, alpha = .75, fill = "green") +
  annotate("text", x = 8, y = 2, label = "virginica", hjust = 1, vjust = 0, fill = "green", size = 3)
```

### How deep?

If we keep increasing the depth of the tree, we will eventually overfit the training data.

```{r}
ctrl <- list(cp = 0, minbucket = 1, maxdepth = 50)
fit <- rpart(y ~ x, data = df, control = ctrl)
rpart.plot(fit)
```

```{r}
df %>%
  mutate(pred = predict(fit, df)) %>%
  ggplot(aes(x, y)) +
  geom_point(alpha = .2, size = 1) +
  geom_line(aes(x, y = truth), color = "blue", size = 0.75) +
  geom_line(aes(y = pred), color = "red", size = 0.75)
```

There are two basic strategies for finding the optimal depth of the tree, early stopping and pruning:


```{r}
hyper.grid <- expand.grid(
   maxdepth = c(1, 5, 15),
   minbucket = c(1, 5, 15)
)

results <- data.frame(NULL)

for(i in seq_len(nrow(hyper.grid))) {
   ctrl <- list(cp = 0, maxdepth = hyper.grid$maxdepth[i], minbucket = hyper.grid$minbucket[i])
   fit <- rpart(y ~ x, data = df, control = ctrl)
   
   predictions <- mutate(
      df,
      minbucket = factor(paste("Min node size =", hyper.grid$minbucket[i]), ordered = T),
      maxdepth = factor(paste("Max tree depth =", hyper.grid$maxdepth[i]), ordered = T)
   )
   
   predictions$pred <- predict(fit, df)
   results <- rbind(results, predictions)
}

ggplot(results, aes(x, y)) +
   geom_point(alpha = .2, size = 1) +
   geom_line(aes(x, y = truth), color = "blue", size = .75) +
   geom_line(aes(y = pred), color = "red", size = 1) +
   facet_grid(minbucket ~ maxdepth)
```

### Pruning

Alternative to specifying a max depth, build the most complicated tree and then prune it back for generalizability.

We find the optimal subtree by using a _cost complexity parameter_ ($\alpha$) that penalizes our objective function:

$minimize\{SST+\alpha|T|\}$

(Similar to Lasso regression)

```{r}
ctrl <- list(cp = 0, minbucket = 1, maxdepth = 50)
fit <- rpart(y ~ x, data = df, control = ctrl)

p1 <- df %>%
  mutate(pred = predict(fit, df)) %>%
  ggplot(aes(x, y)) +
  geom_point(alpha = .3, size = 2) +
  geom_line(aes(x, y = truth), color = "blue", size = 1) +
  geom_line(aes(y = pred), color = "red", size = 1)

fit2 <- rpart(y ~ x, data = df)

p2 <- df %>%
  mutate(pred2 = predict(fit2, df)) %>%
  ggplot(aes(x, y)) +
  geom_point(alpha = .3, size = 2) +
  geom_line(aes(x, y = truth), color = "blue", size = 1) +
  geom_line(aes(y = pred2), color = "red", size = 1)

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

### Example: AMES Housing Data

```{r}
ames.dt1 <- rpart(
   formula = Sale_Price ~ .,
   data = ames.train,
   method = "anova"
)

ames.dt1

rpart.plot(ames.dt1)
```

```{r}
plotcp(ames.dt1)
```

```{r}
ames.dt2 <- rpart(
    formula = Sale_Price ~ .,
    data    = ames.train,
    method  = "anova", 
    control = list(cp = 0, xval = 10)
)

plotcp(ames.dt2)
abline(v = 11, lty = "dashed")
```

```{r}
ames.dt1$cptable
```

Cross-validated parameter search:

```{r}
ames.dt3 <- train(
   Sale_Price ~ .,
   data = ames.train,
   method = "rpart",
   trControl = trainControl(method = "cv", number = 10),
   tuneLength = 20
)

ggplot(ames.dt3)
```

### Feature Interpretation

```{r}
vip(ames.dt3, num_features = 40, geom = "point")
```

```{r}
# Construct partial dependence plots
p1 <- pdp::partial(ames.dt3, pred.var = "Gr_Liv_Area") %>% autoplot()
p2 <- pdp::partial(ames.dt3, pred.var = "Year_Built") %>% autoplot()
p3 <- pdp::partial(ames.dt3, pred.var = c("Gr_Liv_Area", "Year_Built")) %>% 
  plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, 
              colorkey = TRUE, screen = list(z = -20, x = -60))

# Display plots side by side
gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
```

```{r}
# clean up
rm(list = ls())
```

