---
title: ''
mainfont: Arial
fontsize: 12pt
fig_width: 9
fig_height: 3.5
documentclass: report
header-includes:
- \PassOptionsToPackage{table}{xcolor}
- \usepackage{caption}
- \usepackage{amssymb}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage[table]{xcolor}
- \usepackage{fancyhdr}
- \usepackage{boldline}
- \usepackage{tipa}
   \definecolor{headergrey}{HTML}{545454}
   \definecolor{msdblue}{HTML}{1C93D1}
   \pagestyle{fancy}
   \setlength\headheight{30pt}
   \rhead{\color{headergrey}\today}
   \fancyhead[L]{\color{headergrey}Moretz, Brandon}
   \fancyhead[C]{\Large\bfseries\color{headergrey}Statistical Learning}
   \rfoot{\color{headergrey}\thepage}
   \lfoot{\color{headergrey}Chapter 5}
   \fancyfoot[C]{\rmfamily\color{headergrey}Resampling Methods}
geometry: left = 1cm, right = 1cm, top = 2cm, bottom = 3cm
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    fig_caption: yes
    latex_engine: xelatex
editor_options: 
  chunk_output_type: console
---


```{r knitr_setup, include = FALSE}

knitr::opts_chunk$set(
   echo = T, 
   eval = TRUE, 
   dev = 'png', 
   fig.width = 9, 
   fig.height = 3.5)

options(knitr.table.format = "latex")

```

```{r report_setup, message = FALSE, warning = FALSE, include = FALSE}

# Data Wrangling

library(data.table, quietly = TRUE, warn.conflicts = FALSE)
library(dplyr, quietly = TRUE, warn.conflicts = FALSE)
library(tinytex, quietly = TRUE, warn.conflicts = FALSE)
library(stringr, quietly = TRUE, warn.conflicts = FALSE)
library(lubridate, quietly = TRUE, warn.conflicts = FALSE)
library(reshape2, quietly = TRUE, warn.conflicts = FALSE)
library(ggrepel, quietly = TRUE, warn.conflicts = FALSE)

# Plotting / Graphics

library(ggplot2, quietly = TRUE, warn.conflicts = FALSE)
library(ggrepel, quietly = TRUE, warn.conflicts = FALSE)
library(ggthemes, quietly = TRUE, warn.conflicts = FALSE)
library(GGally, quietly = TRUE, warn.conflicts = FALSE)
library(grid, quietly = TRUE, warn.conflicts = FALSE)
library(gridExtra, quietly = TRUE, warn.conflicts = FALSE)
library(png, quietly = TRUE, warn.conflicts = FALSE)
library(extrafont, quietly = TRUE, warn.conflicts = FALSE)

# Formatting / Markdown

library(knitr, quietly = TRUE, warn.conflicts = FALSE)
library(kableExtra, quietly = TRUE, warn.conflicts = FALSE)
library(scales, quietly = TRUE, warn.conflicts = FALSE)
library(pander, quietly = TRUE, warn.conflicts = FALSE)
library(formattable, quietly = TRUE, warn.conflicts = FALSE)

# Utility
library(here, quietly = TRUE, warn.conflicts = FALSE)

# Resampling & Modeling
library(car, quietly = TRUE, warn.conflicts = FALSE)
library(MASS, quietly = TRUE, warn.conflicts = FALSE)
library(ISLR, quietly = TRUE, warn.conflicts = FALSE)
library(rsample, quietly = TRUE, warn.conflicts = FALSE)
library(caret, quietly = TRUE, warn.conflicts = FALSE)
library(class, quietly = TRUE, warn.conflicts = FALSE)

options(tinytex.verbose = TRUE)
suppressMessages(library("tidyverse"))

pretty_kable <- function(data, title, dig = 2) {
  kable(data, caption = title, digits = dig) %>%
    kable_styling(bootstrap_options = c("striped", "hover")) %>%
      kableExtra::kable_styling(latex_options = "hold_position")
}

theme_set(theme_light())

# Theme Overrides
theme_update(axis.text.x = element_text(size = 10),
             axis.text.y = element_text(size = 10),
             plot.title = element_text(hjust = 0.5, size = 16, face = "bold", color = "darkgreen"),
             axis.title = element_text(face = "bold", size = 12, colour = "steelblue4"),
             plot.subtitle = element_text(face = "bold", size = 8, colour = "darkred"),
             legend.title = element_text(size = 12, color = "darkred", face = "bold"),
             legend.position = "right", legend.title.align=0.5,
             panel.border = element_rect(linetype = "solid", 
                                         colour = "lightgray"), 
             plot.margin = unit(c( 0.1, 0.1, 0.1, 0.1), "inches"))

data.dir <- paste0(here::here(), "/datasets/")

select <- dplyr::select
```

## Chapter 5

```{r pander_setup, include = FALSE}

knitr::opts_chunk$set(comment = NA)

panderOptions('table.alignment.default', function(df)
    ifelse(sapply(df, is.numeric), 'right', 'left'))
panderOptions('table.split.table', Inf)
panderOptions('big.mark', ",")
panderOptions('keep.trailing.zeros', TRUE)

```

### Notes

#### Cross-Validation

Cross-validation is the process of splitting the training data into multiple subsets that can be used for evaluating the out-of-sample performance of a statistical model.

There are multiple strategies for this technique.

#### The Validation Set approach

The most basic strategy for CV is a training/test split of the sample.

Additionally, stratification can be used to ensure even splitting when the response variable is unevenly distributed in the sample (low response logistic regression, for example).

Using Auto:

```{r}
auto <- data.table(ISLR::Auto)
```

```{r}
models <- list()

base.model <- "mpg ~ horsepower"

prev <- ""
for(term in 1:10)
{
   cur <- ifelse(term > 1, paste(prev, rep(paste0("+ I(horsepower^", term,")"))), "")
   fmla <- as.formula(paste0(base.model, cur))
   
   train.error <- numeric(10); test.error <- numeric(10)
   
   for(iter in 1:10)
   {
      auto.split <- initial_split(auto, prop = .5)
      
      auto.train <- training(auto.split)
      model <- lm(fmla, data = auto.train)
      train.error[iter] <- mean(model$residuals^2)
      
      auto.test <- testing(auto.split)
      auto.test$pred <- predict(model, newdata = auto.test)
      test.error[iter] <- with(auto.test, mean( (mpg - pred)^2 ) )
   }
   
   models[[term]] <- list(terms = term, train.error = train.error, test.error = test.error)
   prev <- cur
}

auto.fits <- rbindlist(models, fill = F)

p1 <- auto.fits %>%
   group_by(terms) %>%
   summarise(mean_error = mean(train.error)) %>%
   ggplot(., aes(terms, mean_error)) +
      geom_point(col = "cornflowerblue") +
      geom_line(alpha = .15) +
      labs(title = "Horsepower Model vs Polynomial Degree", x = "Polynomial Degree", y = "Avg. Training MSE") +
      scale_y_continuous(limits = c(10, 30))

p2 <- auto.fits[, .(num = 1:.N, train.error), by = list(terms)] %>%
   ggplot() +
   geom_line(aes(num, train.error, group = terms, col = terms)) +
   labs(title = "Train Sample vs Error (MSE)", x = "Sample Number", y = "Training MSE") +
   theme(legend.position = "none")

gridExtra::grid.arrange(p1, p2, nrow = 1)

ggplot(auto.fits[, .(train = mean(train.error), test = mean(test.error)), by = list(terms)]) +
   geom_point(aes(terms, train), col = "cornflowerblue") +
   geom_point(aes(terms, test), col = "darkorange") +
   labs(x = "Polynomial Terms", y = "Avg. Error")

p1 <- ggplot(auto.fits) +
   geom_boxplot(aes(y = train.error, group = terms, fill = terms))

p2 <- ggplot(auto.fits) +
   geom_boxplot(aes(y = test.error, group = terms, fill = terms))

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

Drawbacks:

+ 1.) The validation estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are in the validation set.

+ 2.) In the validation approach, only a subset of the observations - those that are included in the training set rather than the validation set - are used to fit the model. This can lead to overestimation of model performance.


#### Leave-One-Out Cross-Validation

Leave-one-out cross-validation (LOOCV) is closely related to the validation set approach, but attempts to address some of the drawbacks.

The basic premise of LOOCV is to leave exactly 1 observation out of the data to test against, and use the remaining n-1 observations to train the model.

After each resample, the final test error is produced by:

$CV_{(n)} = \frac{1}{n}\sum^{n}_{i=1}{MSE_i}$

This is more of systematic approach to model training/validation.

Advantages:

+ Far less bias than the validation/test set.

#### k-Fold Cross-Validation

An alternative to LOOCV is k-Fold CV. This approach randomly divides the set of observations into *k* groups, or *k*-folds, of approximately equal size.

The first fold is treated as a validation set, and the method is fit on the remaining *k-1* folds. LOOCV is a special case of k-Fold, (k = n).

However, there is a bias/variance trade-off associated with the choice of k. If you have a large number of k (example, n), then each model output will be highly correlated to each other.

Typically, a good choice of k is 5 or 10.

#### Cross-Validation on Classification

Instead of MSE for error metric, we will use:

$CV_{(n)} = \frac{1}{n}\sum^{n}_{i=1}Err_i$

#### The Bootstrap

Bootstrapping is the process of resampling a data set (with replacement) to obtain confidence intervals (SE) for unknown population parameters.

### R lab

