\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[12pt,]{report}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
    \setmainfont[]{Arial}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{hyperref}
\hypersetup{
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage[left = 1cm, right = 1cm, top = 2cm, bottom = 3cm]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\PassOptionsToPackage{table}{xcolor}
\usepackage{caption}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage[table]{xcolor}
\usepackage{fancyhdr}
\usepackage{boldline}
\usepackage{tipa} \definecolor{headergrey}{HTML}{545454} \definecolor{msdblue}{HTML}{1C93D1} \pagestyle{fancy} \setlength\headheight{30pt} \rhead{\color{headergrey}\today} \fancyhead[L]{\color{headergrey}Moretz, Brandon} \fancyhead[C]{\Large\bfseries\color{headergrey}Statistical Learning} \rfoot{\color{headergrey}\thepage} \lfoot{\color{headergrey}Chapter 5} \fancyfoot[C]{\rmfamily\color{headergrey}Resampling Methods}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

\author{}
\date{\vspace{-2.5em}20 January, 2020}

\begin{document}

\hypertarget{chapter-5}{%
\section{Chapter 5}\label{chapter-5}}

\hypertarget{notes}{%
\subsection{Notes}\label{notes}}

\hypertarget{cross-validation}{%
\subsubsection{Cross-Validation}\label{cross-validation}}

Cross-validation is the process of splitting the training data into
multiple subsets that can be used for evaluating the out-of-sample
performance of a statistical model.

There are multiple strategies for this technique.

\hypertarget{the-validation-set-approach}{%
\subsubsection{The Validation Set
approach}\label{the-validation-set-approach}}

The most basic strategy for CV is a training/test split of the sample.

Additionally, stratification can be used to ensure even splitting when
the response variable is unevenly distributed in the sample (low
response logistic regression, for example).

Using Auto:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{auto <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(ISLR}\OperatorTok{::}\NormalTok{Auto)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{models <-}\StringTok{ }\KeywordTok{list}\NormalTok{()}

\NormalTok{base.model <-}\StringTok{ "mpg ~ horsepower"}

\NormalTok{prev <-}\StringTok{ ""}
\ControlFlowTok{for}\NormalTok{(term }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{)}
\NormalTok{\{}
\NormalTok{   cur <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(term }\OperatorTok{>}\StringTok{ }\DecValTok{1}\NormalTok{, }\KeywordTok{paste}\NormalTok{(prev, }\KeywordTok{rep}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{"+ I(horsepower^"}\NormalTok{, term,}\StringTok{")"}\NormalTok{))), }\StringTok{""}\NormalTok{)}
\NormalTok{   fmla <-}\StringTok{ }\KeywordTok{as.formula}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(base.model, cur))}
   
\NormalTok{   train.error <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\DecValTok{10}\NormalTok{); test.error <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\DecValTok{10}\NormalTok{)}
   
   \ControlFlowTok{for}\NormalTok{(iter }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{)}
\NormalTok{   \{}
\NormalTok{      auto.split <-}\StringTok{ }\KeywordTok{initial_split}\NormalTok{(auto, }\DataTypeTok{prop =} \FloatTok{.5}\NormalTok{)}
      
\NormalTok{      auto.train <-}\StringTok{ }\KeywordTok{training}\NormalTok{(auto.split)}
\NormalTok{      model <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(fmla, }\DataTypeTok{data =}\NormalTok{ auto.train)}
\NormalTok{      train.error[iter] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(model}\OperatorTok{$}\NormalTok{residuals}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
      
\NormalTok{      auto.test <-}\StringTok{ }\KeywordTok{testing}\NormalTok{(auto.split)}
\NormalTok{      auto.test}\OperatorTok{$}\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(model, }\DataTypeTok{newdata =}\NormalTok{ auto.test)}
\NormalTok{      test.error[iter] <-}\StringTok{ }\KeywordTok{with}\NormalTok{(auto.test, }\KeywordTok{mean}\NormalTok{( (mpg }\OperatorTok{-}\StringTok{ }\NormalTok{pred)}\OperatorTok{^}\DecValTok{2}\NormalTok{ ) )}
\NormalTok{   \}}
   
\NormalTok{   models[[term]] <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{terms =}\NormalTok{ term, }\DataTypeTok{train.error =}\NormalTok{ train.error, }\DataTypeTok{test.error =}\NormalTok{ test.error)}
\NormalTok{   prev <-}\StringTok{ }\NormalTok{cur}
\NormalTok{\}}

\NormalTok{auto.fits <-}\StringTok{ }\KeywordTok{rbindlist}\NormalTok{(models, }\DataTypeTok{fill =}\NormalTok{ F)}

\NormalTok{p1 <-}\StringTok{ }\NormalTok{auto.fits }\OperatorTok{%>%}
\StringTok{   }\KeywordTok{group_by}\NormalTok{(terms) }\OperatorTok{%>%}
\StringTok{   }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{mean_error =} \KeywordTok{mean}\NormalTok{(train.error)) }\OperatorTok{%>%}
\StringTok{   }\KeywordTok{ggplot}\NormalTok{(., }\KeywordTok{aes}\NormalTok{(terms, mean_error)) }\OperatorTok{+}
\StringTok{      }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{col =} \StringTok{"cornflowerblue"}\NormalTok{) }\OperatorTok{+}
\StringTok{      }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{.15}\NormalTok{) }\OperatorTok{+}
\StringTok{      }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"Horsepower Model vs Polynomial Degree"}\NormalTok{, }\DataTypeTok{x =} \StringTok{"Polynomial Degree"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Avg. Training MSE"}\NormalTok{) }\OperatorTok{+}
\StringTok{      }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{limits =} \KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{30}\NormalTok{))}

\NormalTok{p2 <-}\StringTok{ }\NormalTok{auto.fits[, .(}\DataTypeTok{num =} \DecValTok{1}\OperatorTok{:}\NormalTok{.N, train.error), by =}\StringTok{ }\KeywordTok{list}\NormalTok{(terms)] }\OperatorTok{%>%}
\StringTok{   }\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\StringTok{   }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(num, train.error, }\DataTypeTok{group =}\NormalTok{ terms, }\DataTypeTok{col =}\NormalTok{ terms)) }\OperatorTok{+}
\StringTok{   }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"Train Sample vs Error (MSE)"}\NormalTok{, }\DataTypeTok{x =} \StringTok{"Sample Number"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Training MSE"}\NormalTok{) }\OperatorTok{+}
\StringTok{   }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"none"}\NormalTok{)}

\NormalTok{gridExtra}\OperatorTok{::}\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{nrow =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{05_Resampling-Methods_files/figure-latex/unnamed-chunk-2-1.png}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(auto.fits[, .(}\DataTypeTok{train =} \KeywordTok{mean}\NormalTok{(train.error), }\DataTypeTok{test =} \KeywordTok{mean}\NormalTok{(test.error)), }\DataTypeTok{by =} \KeywordTok{list}\NormalTok{(terms)]) }\OperatorTok{+}
\StringTok{   }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(terms, train), }\DataTypeTok{col =} \StringTok{"cornflowerblue"}\NormalTok{) }\OperatorTok{+}
\StringTok{   }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(terms, test), }\DataTypeTok{col =} \StringTok{"darkorange"}\NormalTok{) }\OperatorTok{+}
\StringTok{   }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Polynomial Terms"}\NormalTok{, }\DataTypeTok{y =} \StringTok{"Avg. Error"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{05_Resampling-Methods_files/figure-latex/unnamed-chunk-2-2.png}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(auto.fits) }\OperatorTok{+}
\StringTok{   }\KeywordTok{geom_boxplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ train.error, }\DataTypeTok{group =}\NormalTok{ terms, }\DataTypeTok{fill =}\NormalTok{ terms))}

\NormalTok{p2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(auto.fits) }\OperatorTok{+}
\StringTok{   }\KeywordTok{geom_boxplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ test.error, }\DataTypeTok{group =}\NormalTok{ terms, }\DataTypeTok{fill =}\NormalTok{ terms))}

\NormalTok{gridExtra}\OperatorTok{::}\KeywordTok{grid.arrange}\NormalTok{(p1, p2, }\DataTypeTok{nrow =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{05_Resampling-Methods_files/figure-latex/unnamed-chunk-2-3.png}

Drawbacks:

\begin{itemize}
\item
  1.) The validation estimate of the test error rate can be highly
  variable, depending on precisely which observations are included in
  the training set and which observations are in the validation set.
\item
  2.) In the validation approach, only a subset of the observations -
  those that are included in the training set rather than the validation
  set - are used to fit the model. This can lead to overestimation of
  model performance.
\end{itemize}

\hypertarget{leave-one-out-cross-validation}{%
\subsubsection{Leave-One-Out
Cross-Validation}\label{leave-one-out-cross-validation}}

Leave-one-out cross-validation (LOOCV) is closely related to the
validation set approach, but attempts to address some of the drawbacks.

The basic premise of LOOCV is to leave exactly 1 observation out of the
data to test against, and use the remaining n-1 observations to train
the model.

After each resample, the final test error is produced by:

\(CV_{(n)} = \frac{1}{n}\sum^{n}_{i=1}{MSE_i}\)

This is more of systematic approach to model training/validation.

Advantages:

\begin{itemize}
\tightlist
\item
  Far less bias than the validation/test set.
\end{itemize}

\hypertarget{k-fold-cross-validation}{%
\subsubsection{k-Fold Cross-Validation}\label{k-fold-cross-validation}}

An alternative to LOOCV is k-Fold CV. This approach randomly divides the
set of observations into \emph{k} groups, or \emph{k}-folds, of
approximately equal size.

The first fold is treated as a validation set, and the method is fit on
the remaining \emph{k-1} folds. LOOCV is a special case of k-Fold, (k =
n).

However, there is a bias/variance trade-off associated with the choice
of k. If you have a large number of k (example, n), then each model
output will be highly correlated to each other.

Typically, a good choice of k is 5 or 10.

\hypertarget{cross-validation-on-classification}{%
\subsubsection{Cross-Validation on
Classification}\label{cross-validation-on-classification}}

Instead of MSE for error metric, we will use:

\(CV_{(n)} = \frac{1}{n}\sum^{n}_{i=1}Err_i\)

\hypertarget{the-bootstrap}{%
\subsubsection{The Bootstrap}\label{the-bootstrap}}

Bootstrapping is the process of resampling a data set (with replacement)
to obtain confidence intervals (SE) for unknown population parameters.

\hypertarget{r-lab}{%
\subsection{R lab}\label{r-lab}}

\hypertarget{the-validation-set-approach-1}{%
\subsubsection{The Validation Set
Approach}\label{the-validation-set-approach-1}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\NormalTok{train <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{392}\NormalTok{, }\DecValTok{196}\NormalTok{)}

\NormalTok{lm.fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(mpg }\OperatorTok{~}\StringTok{ }\NormalTok{horsepower, }\DataTypeTok{data =}\NormalTok{ auto, }\DataTypeTok{subset =}\NormalTok{ train)}

\KeywordTok{mean}\NormalTok{(lm.fit}\OperatorTok{$}\NormalTok{residuals}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\CommentTok{# train MSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 25.05959
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test <-}\StringTok{ }\NormalTok{auto[}\OperatorTok{!}\NormalTok{train]}
\NormalTok{test}\OperatorTok{$}\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(lm.fit, }\DataTypeTok{newdata =}\NormalTok{ test)}
\KeywordTok{with}\NormalTok{(test, }\KeywordTok{mean}\NormalTok{((mpg }\OperatorTok{-}\StringTok{ }\NormalTok{pred)}\OperatorTok{^}\DecValTok{2}\NormalTok{)) }\CommentTok{# test MSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 23.26601
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.fit2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(mpg }\OperatorTok{~}\StringTok{ }\KeywordTok{poly}\NormalTok{(horsepower, }\DecValTok{2}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ auto, }\DataTypeTok{subset =}\NormalTok{ train)}
\NormalTok{test}\OperatorTok{$}\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(lm.fit2, }\DataTypeTok{newdata =}\NormalTok{ test) }\CommentTok{# update predictions}
\KeywordTok{with}\NormalTok{(test, }\KeywordTok{mean}\NormalTok{((mpg }\OperatorTok{-}\StringTok{ }\NormalTok{pred)}\OperatorTok{^}\DecValTok{2}\NormalTok{)) }\CommentTok{# model 2 MSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 18.71646
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm.fit3 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(mpg }\OperatorTok{~}\StringTok{ }\KeywordTok{poly}\NormalTok{(horsepower, }\DecValTok{3}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ auto, }\DataTypeTok{subset =}\NormalTok{ train)}
\NormalTok{test}\OperatorTok{$}\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(lm.fit3, }\DataTypeTok{newdata =}\NormalTok{ test) }\CommentTok{# update predictions}
\KeywordTok{with}\NormalTok{(test, }\KeywordTok{mean}\NormalTok{((mpg }\OperatorTok{-}\StringTok{ }\NormalTok{pred)}\OperatorTok{^}\DecValTok{2}\NormalTok{)) }\CommentTok{# model 2 MSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 18.79401
\end{verbatim}

\hypertarget{leave-one-out-cv}{%
\subsubsection{Leave-One-Out CV}\label{leave-one-out-cv}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glm.fit <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(mpg }\OperatorTok{~}\StringTok{ }\NormalTok{horsepower, }\DataTypeTok{data =}\NormalTok{ auto)}
\KeywordTok{coef}\NormalTok{(glm.fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(Intercept)  horsepower 
 39.9358610  -0.1578447 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{glm.fit <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(mpg }\OperatorTok{~}\StringTok{ }\NormalTok{horsepower, }\DataTypeTok{data =}\NormalTok{ auto)}
\NormalTok{cv.err <-}\StringTok{ }\KeywordTok{cv.glm}\NormalTok{(auto, glm.fit)}

\NormalTok{cv.err}\OperatorTok{$}\NormalTok{delta}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 24.23151 24.23114
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cv.error <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{( i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{ )}
\NormalTok{\{}
\NormalTok{   glm.fit <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(mpg }\OperatorTok{~}\StringTok{ }\KeywordTok{poly}\NormalTok{(horsepower, i), }\DataTypeTok{data =}\NormalTok{ auto)}
\NormalTok{   cv.error[i] <-}\StringTok{ }\KeywordTok{cv.glm}\NormalTok{(auto, glm.fit)}\OperatorTok{$}\NormalTok{delta[}\DecValTok{1}\NormalTok{]}
\NormalTok{\}}

\NormalTok{cv.error}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 24.23151 19.24821 19.33498 19.42443 19.03321
\end{verbatim}

\hypertarget{k-fold-cross-validation-1}{%
\subsubsection{k-Fold
Cross-Validation}\label{k-fold-cross-validation-1}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{7}\NormalTok{)}

\NormalTok{cv.error}\FloatTok{.10}\NormalTok{ <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{( i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{ )}
\NormalTok{\{}
\NormalTok{   glm.fit <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(mpg }\OperatorTok{~}\StringTok{ }\KeywordTok{poly}\NormalTok{(horsepower, i), }\DataTypeTok{data =}\NormalTok{ auto)}
\NormalTok{   cv.error}\FloatTok{.10}\NormalTok{[i] <-}\StringTok{ }\KeywordTok{cv.glm}\NormalTok{(auto, glm.fit)}\OperatorTok{$}\NormalTok{delta[}\DecValTok{1}\NormalTok{]}
\NormalTok{\}}

\NormalTok{cv.error}\FloatTok{.10}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] 24.23151 19.24821 19.33498 19.42443 19.03321 18.97864 18.83305 18.96115
 [9] 19.06863 19.49093
\end{verbatim}

\hypertarget{the-bootstrap-1}{%
\subsubsection{The Bootstrap}\label{the-bootstrap-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{portfolio <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(ISLR}\OperatorTok{::}\NormalTok{Portfolio)}

\NormalTok{alpha.fn <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(data, index ) \{}
\NormalTok{   X <-}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{X; Y <-}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{Y}
   \KeywordTok{return}\NormalTok{ ((}\KeywordTok{var}\NormalTok{(Y)}\OperatorTok{-}\KeywordTok{cov}\NormalTok{(X, Y)) }\OperatorTok{/}\StringTok{ }\NormalTok{(}\KeywordTok{var}\NormalTok{(X) }\OperatorTok{+}\StringTok{ }\KeywordTok{var}\NormalTok{(Y) }\OperatorTok{-}\StringTok{ }\DecValTok{2}\OperatorTok{*}\KeywordTok{cov}\NormalTok{(X, Y)))}
\NormalTok{\}}

\KeywordTok{alpha.fn}\NormalTok{(portfolio, }\DecValTok{1}\OperatorTok{:}\DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5758321
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\KeywordTok{alpha.fn}\NormalTok{(portfolio, }\KeywordTok{sample}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DataTypeTok{replace =}\NormalTok{ T))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5758321
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{boot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ portfolio, }\DataTypeTok{statistic =}\NormalTok{ alpha.fn, }\DataTypeTok{R =} \DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = portfolio, statistic = alpha.fn, R = 1000)


Bootstrap Statistics :
     original  bias    std. error
t1* 0.5758321       0           0
\end{verbatim}

\hypertarget{estimating-the-accuracy-of-a-linear-regression-model}{%
\subsubsection{Estimating the Accuracy of a Linear Regression
Model}\label{estimating-the-accuracy-of-a-linear-regression-model}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boot.fn <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(data, index) \{}
   \KeywordTok{return}\NormalTok{ (}\KeywordTok{coef}\NormalTok{(}\KeywordTok{lm}\NormalTok{(mpg }\OperatorTok{~}\StringTok{ }\NormalTok{horsepower, }\DataTypeTok{data =}\NormalTok{ data, }\DataTypeTok{subset =}\NormalTok{ index)))}
\NormalTok{\}}

\KeywordTok{boot.fn}\NormalTok{(auto, }\DecValTok{1}\OperatorTok{:}\DecValTok{396}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(Intercept)  horsepower 
 39.9358610  -0.1578447 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\KeywordTok{boot.fn}\NormalTok{(auto, }\KeywordTok{sample}\NormalTok{(}\DecValTok{392}\NormalTok{, }\DecValTok{392}\NormalTok{, }\DataTypeTok{replace =}\NormalTok{ T))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(Intercept)  horsepower 
 40.3404517  -0.1634868 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{boot}\NormalTok{(auto, boot.fn, }\DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = auto, statistic = boot.fn, R = 1000)


Bootstrap Statistics :
      original        bias    std. error
t1* 39.9358610  0.0549915227 0.841925746
t2* -0.1578447 -0.0006210818 0.007348956
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(}\KeywordTok{lm}\NormalTok{(mpg }\OperatorTok{~}\StringTok{ }\NormalTok{horsepower, }\DataTypeTok{data =}\NormalTok{ auto))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = mpg ~ horsepower, data = auto)

Residuals:
     Min       1Q   Median       3Q      Max 
-13.5710  -3.2592  -0.3435   2.7630  16.9240 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 39.935861   0.717499   55.66   <2e-16 ***
horsepower  -0.157845   0.006446  -24.49   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.906 on 390 degrees of freedom
Multiple R-squared:  0.6059,    Adjusted R-squared:  0.6049 
F-statistic: 599.7 on 1 and 390 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boot.fn <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(data, index)\{}
   \KeywordTok{coefficients}\NormalTok{(}\KeywordTok{lm}\NormalTok{(mpg }\OperatorTok{~}\StringTok{ }\NormalTok{horsepower }\OperatorTok{+}\StringTok{ }\KeywordTok{I}\NormalTok{(horsepower}\OperatorTok{^}\DecValTok{2}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ data), }\DataTypeTok{subset =}\NormalTok{ index)}
\NormalTok{\}}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\KeywordTok{boot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ auto, }\DataTypeTok{statistic =}\NormalTok{ boot.fn, }\DataTypeTok{R =} \DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = auto, statistic = boot.fn, R = 1000)


Bootstrap Statistics :
        original  bias    std. error
t1* 56.900099702       0           0
t2* -0.466189630       0           0
t3*  0.001230536       0           0
\end{verbatim}

\hypertarget{exercises}{%
\subsection{Exercises}\label{exercises}}

\hypertarget{using-the-statistical-properties-of-the-variance-as-well-as-single-variable-calculus-derive-5.6.-in-other-words-prove-that-alpha-given-by-5.6-does-indeed-minimize-varalphax-1---alphay}{%
\subsection{\texorpdfstring{1.) Using the statistical properties of the
variance as well as single variable calculus, derive 5.6. In other
words, prove that \(\alpha\) given by 5.6 does indeed minimize
\(Var(\alphaX + (1 - \alpha)Y\)}{1.) Using the statistical properties of the variance as well as single variable calculus, derive 5.6. In other words, prove that \textbackslash{}alpha given by 5.6 does indeed minimize Var(\textbackslash{}alphaX + (1 - \textbackslash{}alpha)Y}}\label{using-the-statistical-properties-of-the-variance-as-well-as-single-variable-calculus-derive-5.6.-in-other-words-prove-that-alpha-given-by-5.6-does-indeed-minimize-varalphax-1---alphay}}

\end{document}
